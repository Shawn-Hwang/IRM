tokenizer_type: hf
tokenizer_path: ('meta-llama/Llama-2-7b-chat-hf',)
pad_id: -1
vocab_size: -1
IRM_layers: [31]
loss_function_index: 0
default_root_dir: /home/huang717/DRAGN/IRM/injectable-alignment-model/runs/Llama-2-7b-chat-hf_tiny_shakespeare.txt_31_training/
checkpoint_path: /home/huang717/DRAGN/IRM/injectable-alignment-model/default_checkpoints/Llama-2-7b-chat-hf.ckpt
dataset_dir: /home/huang717/DRAGN/IRM/injectable-alignment-model/datasets/TinyShakespeare/
dataset_name: tiny_shakespeare
accelerator: gpu
num_nodes: 1
num_workers: 1
devices: 2
use_slurm: true
log_every_n_steps: 200
check_val_every_n_epoch: 1
val_check_interval: 0.25
batch_size: 8
gradient_accumulation_steps: 1
num_epochs: 1
lr: 0.0001
gamma: 0.85
seed: 42
early_stopping: 1
save_top_k: 3
save_predictions_during_training: true
regularize_loss: False
max_gen_len: 1024
do_logging: False
experiment_name: Llama-2-7b-chat-hf_tiny_shakespeare.txt_31_training
from_pretrained: true
model_name: meta-llama/Llama-2-7b-chat-hf

model_config:
  attention_bias: false
  attention_dropout: 0.0
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: silu
  hidden_size: 4096
  initializer_range: 0.02
  intermediate_size: 11008
  max_position_embeddings: 128
  model_type: llama
  num_attention_heads: 32
  num_hidden_layers: 32
  num_key_value_heads: 32
  pretraining_tp: 1
  rms_norm_eps: .00001
  rope_scaling: ~
  rope_theta: 10000.0
  tie_word_embeddings: false
  torch_dtype: float16
  transformers_version: 4.38.2
  use_cache: true
  vocab_size: 32000
